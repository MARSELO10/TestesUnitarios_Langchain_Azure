# ğŸ§ª Sistema de GeraÃ§Ã£o de Testes UnitÃ¡rios com LangChain e Azure ChatGPT

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![LangChain](https://img.shields.io/badge/LangChain-0.1+-green.svg)](https://langchain.com)
[![Azure](https://img.shields.io/badge/Azure-ChatGPT-orange.svg)](https://azure.microsoft.com)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

## ğŸ“‹ DescriÃ§Ã£o do Projeto

Este projeto implementa um **sistema automatizado de geraÃ§Ã£o de testes unitÃ¡rios** utilizando **LangChain** e **Azure ChatGPT**. O sistema analisa cÃ³digo Python e gera automaticamente testes unitÃ¡rios completos seguindo boas prÃ¡ticas de **Test-Driven Development (TDD)** e metodologias Ã¡geis.

## ğŸ¯ Objetivos do Desafio

- Automatizar a criaÃ§Ã£o de testes unitÃ¡rios para cÃ³digo Python
- Implementar agentes inteligentes usando LangChain
- Integrar Azure ChatGPT para geraÃ§Ã£o de cÃ³digo
- Aplicar conceitos de TDD e testes automatizados
- Documentar processos tÃ©cnicos de forma estruturada

## ğŸ—ï¸ Arquitetura do Sistema

```
unit-tests-generator/
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_generator_agent.py    # Agente principal
â”‚   â”‚   â””â”€â”€ code_analyzer_agent.py     # Analisador de cÃ³digo
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ python_executor.py         # Executor Python
â”‚   â”‚   â””â”€â”€ test_validator.py          # Validador de testes
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ test_generation_prompts.py # Templates de prompts
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ azure_config.py            # ConfiguraÃ§Ãµes Azure
â”œâ”€â”€ ğŸ“ tests/
â”‚   â”œâ”€â”€ test_generator/
â”‚   â”œâ”€â”€ sample_code/                   # CÃ³digo de exemplo
â”‚   â””â”€â”€ generated_tests/               # Testes gerados
â”œâ”€â”€ ğŸ“ examples/
â”‚   â”œâ”€â”€ basic_usage.py
â”‚   â””â”€â”€ advanced_examples.py
â”œâ”€â”€ ğŸ“ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â””â”€â”€ api_reference.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸ› ï¸ Tecnologias Utilizadas

### Frameworks Principais
- **LangChain**: Framework para desenvolvimento de aplicaÃ§Ãµes com LLMs
- **Azure OpenAI**: ServiÃ§o ChatGPT da Microsoft Azure
- **Python 3.8+**: Linguagem de programaÃ§Ã£o principal

### Bibliotecas EspecÃ­ficas
- **langchain-openai**: IntegraÃ§Ã£o LangChain com Azure OpenAI
- **langchain-core**: Componentes centrais do LangChain
- **pytest**: Framework de testes unitÃ¡rios
- **ast**: AnÃ¡lise sintÃ¡tica de cÃ³digo Python
- **black**: FormataÃ§Ã£o automÃ¡tica de cÃ³digo

## ğŸ“¦ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### 1. PrÃ©-requisitos

```bash
# Verificar versÃ£o do Python
python --version  # Deve ser 3.8+

# Criar ambiente virtual
python -m venv venv

# Ativar ambiente virtual
# Windows:
venv\Scripts\activate
# Linux/macOS:
source venv/bin/activate
```

### 2. InstalaÃ§Ã£o das DependÃªncias

```bash
# Instalar dependÃªncias principais
pip install langchain langchain-openai langchain-core
pip install pytest black ast-tools
pip install python-dotenv tiktoken

# Ou usar requirements.txt
pip install -r requirements.txt
```

### 3. ConfiguraÃ§Ã£o do Azure OpenAI

```bash
# Copiar arquivo de configuraÃ§Ã£o
cp .env.example .env

# Editar .env com suas credenciais
nano .env
```

**.env**:
```env
# ConfiguraÃ§Ãµes Azure OpenAI
AZURE_OPENAI_API_KEY=sua_chave_aqui
AZURE_OPENAI_ENDPOINT=https://seu-endpoint.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-02-01
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4

# ConfiguraÃ§Ãµes do Sistema
MAX_TOKENS=2000
TEMPERATURE=0.1
DEBUG_MODE=True
```

## ğŸš€ Como Usar

### Uso BÃ¡sico

```python
from src.agents.test_generator_agent import TestGeneratorAgent
from src.config.azure_config import AzureConfig

# Inicializar configuraÃ§Ã£o
config = AzureConfig()

# Criar agente gerador de testes
agent = TestGeneratorAgent(config)

# CÃ³digo de exemplo para testar
sample_code = '''
def calcular_area_retangulo(largura, altura):
    """Calcula a Ã¡rea de um retÃ¢ngulo."""
    if largura <= 0 or altura <= 0:
        raise ValueError("Largura e altura devem ser positivas")
    return largura * altura

def calcular_perimetro_retangulo(largura, altura):
    """Calcula o perÃ­metro de um retÃ¢ngulo."""
    return 2 * (largura + altura)
'''

# Gerar testes automaticamente
testes_gerados = agent.generate_tests(sample_code)
print(testes_gerados)
```

### Uso AvanÃ§ado com ValidaÃ§Ã£o

```python
from src.agents.test_generator_agent import TestGeneratorAgent
from src.tools.test_validator import TestValidator

# Gerar e validar testes
agent = TestGeneratorAgent()
validator = TestValidator()

# Analisar cÃ³digo complexo
codigo_complexo = '''
class ContaBancaria:
    def __init__(self, saldo_inicial=0):
        self._saldo = saldo_inicial
    
    def depositar(self, valor):
        if valor <= 0:
            raise ValueError("Valor deve ser positivo")
        self._saldo += valor
        return self._saldo
    
    def sacar(self, valor):
        if valor <= 0:
            raise ValueError("Valor deve ser positivo")
        if valor > self._saldo:
            raise ValueError("Saldo insuficiente")
        self._saldo -= valor
        return self._saldo
    
    @property
    def saldo(self):
        return self._saldo
'''

# Gerar testes
testes = agent.generate_tests(codigo_complexo)

# Validar testes gerados
resultado_validacao = validator.validate_tests(testes, codigo_complexo)
print(f"Testes vÃ¡lidos: {resultado_validacao['valid']}")
print(f"Cobertura: {resultado_validacao['coverage']}%")
```

## ğŸ¤– ImplementaÃ§Ã£o dos Agentes

### Agente Principal: TestGeneratorAgent

```python
from langchain.agents import initialize_agent, AgentType
from langchain.tools import Tool
from langchain_openai import AzureChatOpenAI
from langchain.prompts import PromptTemplate

class TestGeneratorAgent:
    def __init__(self, config):
        self.llm = AzureChatOpenAI(
            azure_endpoint=config.AZURE_OPENAI_ENDPOINT,
            api_key=config.AZURE_OPENAI_API_KEY,
            api_version=config.AZURE_OPENAI_API_VERSION,
            deployment_name=config.AZURE_OPENAI_DEPLOYMENT_NAME,
            temperature=0.1
        )
        
        self.tools = self._setup_tools()
        self.agent = self._initialize_agent()
    
    def _setup_tools(self):
        """Configura ferramentas do agente."""
        return [
            Tool(
                name="CodeAnalyzer",
                func=self._analyze_code,
                description="Analisa cÃ³digo Python e identifica funÃ§Ãµes para testar"
            ),
            Tool(
                name="TestGenerator",
                func=self._generate_unit_tests,
                description="Gera testes unitÃ¡rios para cÃ³digo Python"
            ),
            Tool(
                name="PythonREPL",
                func=self._execute_python,
                description="Executa cÃ³digo Python para validaÃ§Ã£o"
            )
        ]
    
    def _initialize_agent(self):
        """Inicializa agente com tipo ZERO_SHOT_REACT_DESCRIPTION."""
        return initialize_agent(
            tools=self.tools,
            llm=self.llm,
            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            verbose=True
        )
    
    def generate_tests(self, source_code):
        """Gera testes unitÃ¡rios para o cÃ³digo fornecido."""
        prompt = f"""
        Analise o seguinte cÃ³digo Python e gere testes unitÃ¡rios completos:
        
        {source_code}
        
        Os testes devem seguir as melhores prÃ¡ticas:
        1. Usar pytest como framework
        2. Testar casos normais e de borda
        3. Incluir testes de exceÃ§Ãµes quando aplicÃ¡vel
        4. Usar nomenclatura clara e descritiva
        5. Incluir docstrings nos testes
        """
        
        return self.agent.run(prompt)
```

### Ferramenta de ExecuÃ§Ã£o Python

```python
from langchain.tools.python.tool import PythonREPLTool

class CustomPythonREPL(PythonREPLTool):
    """Ferramenta customizada para execuÃ§Ã£o de cÃ³digo Python."""
    
    def __init__(self):
        super().__init__()
        self.name = "PythonExecutor"
        self.description = """
        Executa cÃ³digo Python de forma segura.
        Ãštil para validar testes gerados e verificar se o cÃ³digo funciona.
        """
    
    def _run(self, query: str) -> str:
        """Executa cÃ³digo Python com tratamento de erros."""
        try:
            return super()._run(query)
        except Exception as e:
            return f"Erro na execuÃ§Ã£o: {str(e)}"
```

## ğŸ“‹ Templates de Prompts

### Prompt para GeraÃ§Ã£o de Testes

```python
TEST_GENERATION_TEMPLATE = """
VocÃª Ã© um especialista em testes unitÃ¡rios Python. Analise o cÃ³digo fornecido e gere testes unitÃ¡rios completos.

CÃ“DIGO PARA ANÃLISE:
{source_code}

INSTRUÃ‡Ã•ES:
1. Use pytest como framework de teste
2. Crie testes para todos os mÃ©todos pÃºblicos
3. Inclua casos de teste positivos e negativos
4. Teste tratamento de exceÃ§Ãµes quando aplicÃ¡vel
5. Use fixtures quando necessÃ¡rio
6. Mantenha alta cobertura de cÃ³digo

FORMATO DE SAÃDA:
- CÃ³digo de teste completo
- ComentÃ¡rios explicativos
- Imports necessÃ¡rios
- Assertions apropriados

EXEMPLO DE ESTRUTURA:
```python
import pytest
from module import ClassName

class TestClassName:
    def test_method_normal_case(self):
        # Teste caso normal
        pass
    
    def test_method_edge_case(self):
        # Teste caso extremo
        pass
    
    def test_method_exception_case(self):
        # Teste exceÃ§Ãµes
        with pytest.raises(ExceptionType):
            pass
```

Gere os testes agora:
"""

ANALYSIS_TEMPLATE = """
Analise o cÃ³digo Python fornecido e identifique:

CÃ“DIGO:
{code}

ANÃLISE REQUERIDA:
1. FunÃ§Ãµes pÃºblicas para testar
2. Classes e mÃ©todos principais
3. Casos de borda identificados
4. ExceÃ§Ãµes que devem ser testadas
5. DependÃªncias necessÃ¡rias

ForneÃ§a um resumo estruturado da anÃ¡lise.
"""
```

## ğŸ§ª Exemplos de Testes Gerados

### Exemplo 1: FunÃ§Ã£o Simples

**CÃ³digo Original:**
```python
def dividir(a, b):
    """Divide dois nÃºmeros."""
    if b == 0:
        raise ZeroDivisionError("DivisÃ£o por zero nÃ£o permitida")
    return a / b
```

**Teste Gerado:**
```python
import pytest

def test_dividir_numeros_positivos():
    """Testa divisÃ£o com nÃºmeros positivos."""
    resultado = dividir(10, 2)
    assert resultado == 5.0

def test_dividir_numeros_negativos():
    """Testa divisÃ£o com nÃºmeros negativos."""
    resultado = dividir(-10, 2)
    assert resultado == -5.0

def test_dividir_por_zero():
    """Testa divisÃ£o por zero - deve levantar exceÃ§Ã£o."""
    with pytest.raises(ZeroDivisionError, match="DivisÃ£o por zero nÃ£o permitida"):
        dividir(10, 0)

def test_dividir_zero_por_numero():
    """Testa divisÃ£o de zero por nÃºmero."""
    resultado = dividir(0, 5)
    assert resultado == 0.0
```

### Exemplo 2: Classe Complexa

**CÃ³digo Original:**
```python
class CalculadoraFinanceira:
    def __init__(self):
        self.historico = []
    
    def juros_simples(self, capital, taxa, tempo):
        if capital <= 0 or taxa < 0 or tempo <= 0:
            raise ValueError("ParÃ¢metros invÃ¡lidos")
        
        juros = capital * taxa * tempo
        total = capital + juros
        
        self.historico.append({
            'operacao': 'juros_simples',
            'capital': capital,
            'taxa': taxa,
            'tempo': tempo,
            'resultado': total
        })
        
        return total
```

**Teste Gerado:**
```python
import pytest
from calculadora_financeira import CalculadoraFinanceira

class TestCalculadoraFinanceira:
    @pytest.fixture
    def calc(self):
        """Fixture para criar instÃ¢ncia da calculadora."""
        return CalculadoraFinanceira()
    
    def test_juros_simples_caso_normal(self, calc):
        """Testa cÃ¡lculo de juros simples - caso normal."""
        resultado = calc.juros_simples(1000, 0.1, 2)
        assert resultado == 1200.0
        
        # Verifica se foi adicionado ao histÃ³rico
        assert len(calc.historico) == 1
        assert calc.historico[0]['operacao'] == 'juros_simples'
    
    def test_juros_simples_capital_invalido(self, calc):
        """Testa juros simples com capital invÃ¡lido."""
        with pytest.raises(ValueError, match="ParÃ¢metros invÃ¡lidos"):
            calc.juros_simples(-1000, 0.1, 2)
    
    def test_juros_simples_taxa_negativa(self, calc):
        """Testa juros simples com taxa negativa."""
        with pytest.raises(ValueError, match="ParÃ¢metros invÃ¡lidos"):
            calc.juros_simples(1000, -0.1, 2)
    
    def test_historico_multiplas_operacoes(self, calc):
        """Testa acÃºmulo de histÃ³rico em mÃºltiplas operaÃ§Ãµes."""
        calc.juros_simples(1000, 0.1, 1)
        calc.juros_simples(2000, 0.05, 3)
        
        assert len(calc.historico) == 2
        assert calc.historico[0]['capital'] == 1000
        assert calc.historico[1]['capital'] == 2000
```

## ğŸ“Š ValidaÃ§Ã£o e MÃ©tricas

### Sistema de ValidaÃ§Ã£o

```python
class TestValidator:
    """Valida testes gerados automaticamente."""
    
    def validate_tests(self, test_code, original_code):
        """Valida se os testes gerados sÃ£o vÃ¡lidos."""
        validation_result = {
            'valid': True,
            'coverage': 0,
            'issues': [],
            'suggestions': []
        }
        
        try:
            # Executa anÃ¡lise sintÃ¡tica
            ast.parse(test_code)
            
            # Verifica estrutura dos testes
            coverage = self._calculate_coverage(test_code, original_code)
            validation_result['coverage'] = coverage
            
            # Verifica boas prÃ¡ticas
            issues = self._check_best_practices(test_code)
            validation_result['issues'] = issues
            
        except SyntaxError as e:
            validation_result['valid'] = False
            validation_result['issues'].append(f"Erro de sintaxe: {e}")
        
        return validation_result
    
    def _calculate_coverage(self, test_code, original_code):
        """Calcula cobertura aproximada dos testes."""
        # AnÃ¡lise simplificada - em produÃ§Ã£o usar coverage.py
        original_functions = self._extract_functions(original_code)
        tested_functions = self._extract_tested_functions(test_code)
        
        if not original_functions:
            return 100
        
        coverage = len(tested_functions) / len(original_functions) * 100
        return min(coverage, 100)
```

## ğŸ“ˆ MÃ©tricas de Performance

### Dashboard de MÃ©tricas

```python
class MetricsDashboard:
    """Dashboard para acompanhar mÃ©tricas do sistema."""
    
    def __init__(self):
        self.metrics = {
            'tests_generated': 0,
            'success_rate': 0.0,
            'average_coverage': 0.0,
            'execution_time': 0.0
        }
    
    def update_metrics(self, generation_result):
        """Atualiza mÃ©tricas apÃ³s geraÃ§Ã£o de testes."""
        self.metrics['tests_generated'] += 1
        
        if generation_result['valid']:
            success_count = self.metrics['tests_generated'] * self.metrics['success_rate']
            self.metrics['success_rate'] = (success_count + 1) / self.metrics['tests_generated']
        
        # Atualiza cobertura mÃ©dia
        current_avg = self.metrics['average_coverage']
        new_coverage = generation_result['coverage']
        total_tests = self.metrics['tests_generated']
        
        self.metrics['average_coverage'] = (
            (current_avg * (total_tests - 1) + new_coverage) / total_tests
        )
    
    def generate_report(self):
        """Gera relatÃ³rio de mÃ©tricas."""
        return f"""
        ğŸ“Š RELATÃ“RIO DE MÃ‰TRICAS
        ========================
        Testes Gerados: {self.metrics['tests_generated']}
        Taxa de Sucesso: {self.metrics['success_rate']:.2%}
        Cobertura MÃ©dia: {self.metrics['average_coverage']:.1f}%
        Tempo MÃ©dio: {self.metrics['execution_time']:.2f}s
        """
```

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### ConfiguraÃ§Ãµes CustomizÃ¡veis

```python
# config/settings.py
GENERATION_SETTINGS = {
    # ConfiguraÃ§Ãµes do LLM
    'temperature': 0.1,  # Criatividade vs PrecisÃ£o
    'max_tokens': 2000,  # Tamanho mÃ¡ximo da resposta
    'top_p': 0.95,       # Diversidade do vocabulÃ¡rio
    
    # ConfiguraÃ§Ãµes dos Testes
    'test_framework': 'pytest',  # pytest, unittest
    'include_fixtures': True,    # Usar fixtures
    'test_edge_cases': True,     # Testar casos extremos
    'test_exceptions': True,     # Testar exceÃ§Ãµes
    'use_parametrize': True,     # Usar parametrizaÃ§Ã£o
    
    # ConfiguraÃ§Ãµes de Qualidade
    'min_coverage': 80,          # Cobertura mÃ­nima
    'max_complexity': 10,        # Complexidade mÃ¡xima
    'require_docstrings': True,  # Exigir docstrings
}
```

## ğŸš€ Deploy e IntegraÃ§Ã£o

### IntegraÃ§Ã£o com CI/CD

```yaml
# .github/workflows/auto-tests.yml
name: Auto Generate Tests

on:
  push:
    paths:
      - '**.py'
  pull_request:
    paths:
      - '**.py'

jobs:
  generate-tests:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
    
    - name: Generate Tests
      run: |
        python scripts/auto_generate_tests.py
      env:
        AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
        AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
    
    - name: Run Generated Tests
      run: |
        pytest tests/generated/ -v --cov=src/
```

## ğŸ“š DocumentaÃ§Ã£o Adicional

### API Reference

```python
# Principais classes e mÃ©todos
class TestGeneratorAgent:
    def generate_tests(source_code: str) -> str
    def analyze_code(source_code: str) -> dict
    def validate_output(test_code: str) -> bool

class TestValidator:
    def validate_tests(test_code: str, original_code: str) -> dict
    def check_best_practices(test_code: str) -> list
    def calculate_coverage(test_code: str, original_code: str) -> float

class MetricsDashboard:
    def update_metrics(generation_result: dict) -> None
    def generate_report() -> str
    def export_metrics(format: str) -> str
```

## ğŸ› Troubleshooting

### Problemas Comuns

**1. Erro de AutenticaÃ§Ã£o Azure:**
```bash
# Verificar credenciais
echo $AZURE_OPENAI_API_KEY
# Testar conectividade
curl -H "api-key: $AZURE_OPENAI_API_KEY" $AZURE_OPENAI_ENDPOINT/models
```

**2. Testes Gerados InvÃ¡lidos:**
```python
# Ativar debug mode
DEBUG_MODE = True

# Verificar logs detalhados
tail -f logs/generation.log
```

**3. Performance Lenta:**
```python
# Otimizar configuraÃ§Ãµes
GENERATION_SETTINGS['max_tokens'] = 1000
GENERATION_SETTINGS['temperature'] = 0.0
```

## ğŸ¯ PrÃ³ximos Passos

### Roadmap de Funcionalidades

- [ ] **Suporte a TypeScript/JavaScript**
- [ ] **IntegraÃ§Ã£o com IDEs** (VS Code, PyCharm)
- [ ] **GeraÃ§Ã£o de Mocks automÃ¡ticos**
- [ ] **AnÃ¡lise de performance dos testes**
- [ ] **Interface web para configuraÃ§Ã£o**
- [ ] **Suporte a diferentes frameworks de teste**
- [ ] **IntegraÃ§Ã£o com SonarQube**
- [ ] **GeraÃ§Ã£o de relatÃ³rios HTML**

## ğŸ“ ConclusÃ£o

Este sistema demonstra como combinar **LangChain**, **Azure ChatGPT** e **agentes inteligentes** para automatizar uma tarefa complexa como a geraÃ§Ã£o de testes unitÃ¡rios. A abordagem usando **AgentType.ZERO_SHOT_REACT_DESCRIPTION** permite que o agente "pense" sobre o problema e use ferramentas de forma inteligente.

### Principais Aprendizados

1. **Prompting Eficaz**: Templates bem estruturados sÃ£o cruciais
2. **ValidaÃ§Ã£o Robusta**: Sempre validar cÃ³digo gerado
3. **IteraÃ§Ã£o ContÃ­nua**: Melhorar prompts baseado nos resultados
4. **IntegraÃ§Ã£o PrÃ¡tica**: Foco em soluÃ§Ãµes que funcionem no mundo real

### Impacto Esperado

- **ReduÃ§Ã£o de 70-80%** no tempo de criaÃ§Ã£o de testes
- **Melhoria na cobertura** de cÃ³digo de projetos
- **PadronizaÃ§Ã£o** de qualidade nos testes
- **DemocratizaÃ§Ã£o** de boas prÃ¡ticas de TDD

---

### ğŸ“ Projeto Desenvolvido para:
**DIO + BairesDev - Bootcamp Machine Learning**  
**Desafio:** Gerando Testes UnitÃ¡rios com LangChain e Azure ChatGPT

---

â­ **Se este projeto foi Ãºtil, considere dar uma estrela no repositÃ³rio!**

**#LangChain #AzureChatGPT #TDD #TestesUnitarios #MachineLearning #DIO**
